## Design Questions
---
Build env requires a specific CWD, and action references a file path. Should the file path be relative to the build env's required CWD, or the CWD of the project directory?

---
__Project and Action Contexts:__

Should these be provided as globals, or as function parameters? (Easier to supply project context as globals compared to action context.)

Idea:

Project context is available under global var `PROJECT`, which points to the topmost entry in the "project stack" stored at `cobble.project_stack`.

The "project stack": When first asked to process a project file cobble will push an initial project context onto the stack.  When a `project_dir(...)` function call is made, the `project_dir` function will push a new project context onto the stack before processing the project file in the requested directory.  Before returning, `project_dir` pops the project context off of the stack, so that the correct project context is active while the rest of the original project is processed.

Available project variables:
- `PROJECT.name`
- `PROJECT.dir`
- `PROJECT.build_envs`
- `PROJECT.tasks`
- `PROJECT.subprojects`

Action contexts are passed in as arguments to functions.

Note on actions: if an action depends on a tool or build env, it must be visible to cobble while computing dependencies.  For this reason, an action can't simply be a function that then invokes commands in various build environments.  The build environment used must be visible outside of the function.

Declaring an action as part of a task might look like this:

```lua
task {
    -- ...,
    actions = {
        { build_env = "poetry", exec = function(self, cxt) end }
    }
}
```

Where `act` is a reference to the action object (table) that defined the action, (in this case, `{ build_env = "poetry", exec = function(act, cxt) end }`,) and `cxt` is the action context.

Action context fields:
- `cxt:tool` - A function that will execute the external tool's action
- `cxt:build_env` - A function that will execute the build environment's action
- `cxt.project_dir` - The directory of the project from which the action request originated

---
__Handling CWD:__
CWD should always be the project directory for actions unless specified.  Initial action invocation will set `cxt.project_dir`, which should propagate to subsequent calls to `cxt:exec` from within that action.

---
__WORKSPACE:__ Like `PROJECT`, there's a `WORKSPACE` global var that provides workspace-level information:
- `WORKSPACE.dir`
- `WORKSPACE.vars`

---

How to handle naming conflicts of tasks, build envs, etc. that are generated by task generators in the same project?

Have the ability to define a subproject with a prefix within a project file.  If we do this, a project will need both a (unique) name and an associated directory.

---
__Action Contexts:__
I'm leaning towards making action implementation functions only take in a single argument, the "action context".  (This means that without a "self" argument, creating a function closure using table properties will be less intuitive, and capturing variables in functions with upvalues will probably become the recommended way.)  What does an action context have in it?

```
action_context = {
    cmd = <a function for invoking the "cmd" tool.  A shortcut for "tool.cmd">
    tool = { <invocation functions for all the tools depended on by the action, keyed by tool names> },
    env = { <invocation functions for all the build envs depended on by the action, keyed by names as they appear in the build_env declaration> },
    args = { <args passed into this actions invocation function by the caller> },
    action = <the original action definition, on which you can access any defined properties, even those not explicitly part of the action schema>,
    project = { dir = <the project directory this action was invoked from. Can be used to set CWD for actions.  Cmd action will use this as the default if cwd arg isn't defined> }
    vars = <(TODO) the variables defined either by config, env var, or CLI arg>
}
```

---

__Docker Build Environment:__
I'd like to be able to support using a docker container as a build environment.  That is, installing the environment would amount to creating a docker container and installing any build dependencies into it, and actions run in that build environment would be executed with `docker exec`.  Ideally, you'd also be able to run cobble while already inside the docker environment, (e.g in CI or using devcontainers,) in which case running the action in the build environment would just drop the `docker exec` and run the command directly.

I guess a challenge arises when you account for dependencies on other tasks using other environments.  How do those get executed?  Maybe it's best not to support the "I'm already in a container" scenario.  In CI, you could just run the build as usual with a dind container.  With devcontainers, you can [attach to a running container](https://code.visualstudio.com/docs/devcontainers/attach-container) to do development within the container itself.

---

__Calculated Dependencies:__
I think another challenge will be calculated dependencies.  You can add `calc` as a dependency category, so that you can delegate to tasks to dynamically compute dependencies, but take the scenario where you want to parse a pyproject.toml file to compute dependencies.  The task discovers local path dependencies, and adds those as file dependencies to the poetry install task. But, we also have a docker build task, which requires that the local path dependencies are inside the poetry project directory, which also happens to be the docker context that is used to build the docker image.  So, not only do we need to read the pyproject.toml for local path dependencies, we need to also have the pyproject.toml reference an in-project version of those local archives, and then have a task that will copy them from somewhere into the project.  However, now we have a problem: it's easy enough to calculate the dependencies of the poetry install task, but how do we generate the dependencies of the copy files task(s)?  Do we re-parse the pyproject.toml file, and just apply a different transformation to the result?

One idea: allow "named task dependencies".  Similar to how build environments and tools can be aliased and referenced within the task's action, we could also allow aliasing a task dependency, and outputs of dependencies would be made available in the action context under `c.deps.tasks`.

We could potentially do this with files, too, which could be helpful if we ever support file globs or directories as dependencies.  The alias under `c.deps.files` would give you the list of files that the dependency resolved to.

Allowing access to the output of a dependency task would enable the copy task to have a calc dependency that depends on the calc dependency of the poetry install task.  It could then simply apply a transformation to the file names to derive where the files should be copied from.

Another thought was to allow "task generators", which act similarly to calc deps, but generate whole tasks instead of just dependencies.  These task generators would always have to run up-front, since they could potentially produce new artifact entries.  The question is what they would take as input.  You wouldn't get anything that wasn't available to you in the initial execution of all the project.lua scripts, but there would be an opportunity for declaring dependencies and caching results of the task generator tasks.

---

__Artifacts and Task Outputs:__

One question is, what should be considered a task's artifacts?  I think it would make sense to include both the task's (file) artifacts and action output as the task's artifacts.  For most tasks with file artifacts, the task's action output will be nil.  But, what if the task has both file artifacts and a non-nil action output?  That's fine, though other tasks need a way to make up-to-date checks against the files and action output independently.  For example, if a task depends on a file, (but not the task that has the file as its artifact,) then it should only have to re-run if the file it depends on changes.  Changes in the output of the task that produces the file should not trigger a re-run, since the depending task only declared the file as a dependency.  This is similar to the situation where a task depends on only one of another task's file artifacts.  If one file artifact changes, but the file artifact that the task depends on didn't change, then the task should not be re-run.

---

__CLI Args:__

CLI args should be supported, and can be added after a `--` arg, so that args can be separated from task targets.  CLI args should be available to tasks that declare a dependency on CLI args, but not otherwise.  That way, CLI args can be included in the inputs of a task for the up-to-date check without causing an invocation of run from dirtying the inputs of a task that doesn't make use of the CLI args.  An alternative to including CLI args in the inputs of a task for up-to-date checks, declaring a dependency on CLI args could simply cause a task to always run, since it might seem odd to have subsequent invocations of a task to do nothing, even if they include the same CLI args as the last invocation.

If a task declares a dependency on CLI args, then the CLI args will be passed into the context of the first action of that task as `c.args`.

If a task wants to have some parameterization, then it should use `vars`, which is a concept that hasn't been fully developed here, but there should be a set of `vars` defined by either config, environment variables, or command line, and a task can declare a dependency on individual vars.  The vars should be accessible in the action context as `c.vars`.

---